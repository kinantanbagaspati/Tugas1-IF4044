{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uDup6H5Nakk"
   },
   "source": [
    "# HandsOn Week 6 RDD Spark\n",
    "Selamat datang di HandsOn Week 6, yaitu tentang pemrosesan data terdistribusi menggunakan Spark. Untuk tujuan pembelajaran, seperti biasa, kita akan menggunakan *pseudo-distributed mode* (single node cluster) di VM yang telah disediakan. Dengan kode yang *similar* di cluster komputer dengan *n* workers, maka komputasi akan tersebar ke *n* workers tersebut. Adapun yang akan kita coba kali ini adalah melakukan komputasi menggunakan RDD dan DataFrame. Berikut catatan-catatan yang perlu kamu perhatikan dalam hands-on ini:\n",
    "1. Untuk menjalankan Apache Spark dalam bahasa python di VM, ketikkan perintah ```pyspark``` di terminal.\n",
    "2. Dari semua Milestone, data input yang digunakan adalah data \"purchases.txt\" yang telah diletakkan di HDFS. Oleh karena itu, pastikan hadoop service kamu berjalan (```start-dfs.sh```, ```start-yarn.sh```, ```jps```). Untuk membaca data dari HDFS, lihat kembali di slide perkuliahan.\n",
    "3. Untuk Milestone 1, 2 dan 3, kalian perlu untuk mencatat waktu yang diperlukan saat melakukan MapReduce menggunakan hadoop streaming jar di hands-on sebelumnya. Waktu bisa dihitung dari selisih \"waktu awal\" dan \"waktu akhir\" yang tampak di terminal saat kalian selesain melakukan MapReduce -atau menggunakan cara lain yang masih *acceptable*-. (lihat ilustrasi di bawah).\n",
    "4. Lakukan zip file jupyter notebook ini beserta gambar-gambar yang diperlukan -screenshot waktu proses MapReduce Hadoop jar-, dan submit ke portal kuliah EDUNEX dengan format nama \"**HandsOnWeek10_NIM_NamaLengkap.zip**\". Pastikan file jupyter notebook yang kamu zip dalam kondisi memiliki output per cellnya (tidak kosong karena belum dijalankan). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HJOcG2mNaks"
   },
   "source": [
    "## Milestone 1\n",
    "Kerjakan Milestone 1 pada HandsOn Week 6(sebelumnya), akan tetapi menggunakan RDD Spark. Catat waktu (bandingkan) yang dibutuhkan (dalam detik) antara: \"MapReduce menggunakan hadoop streaming jar\" dengan yang akan kamu proses menggunakan RDD Spark ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "caB7L1cKNaks",
    "outputId": "5f26548d-0307-412c-939b-5ec799f6bab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Consumer Electronics', 57452374.12999931), ('Toys', 57463477.10999949)]\n",
      "Waktu yang diperlukan dengan Hadoop: 55.576\n",
      "Waktu yang diperlukan dengan RDD Spark: 31.404622077941895\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "def m1_mapper(x):\n",
    "    data = x.split(\"\\t\")\n",
    "    try:\n",
    "        if (\"toys\" in data[3].lower()):\n",
    "            return (\"Toys\", float(data[4]))\n",
    "        elif (\"consumer electronics\" in data[3].lower()):\n",
    "            return (\"Consumer Electronics\", float(data[4]))\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "m1_start = time()\n",
    "data = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n",
    "mapped_data = (data.filter(\n",
    "        lambda x: (\"toys\" in x.split(\"\\t\")[3].lower()) or (\"consumer electronics\" in x.split(\"\\t\")[3].lower())\n",
    "    ).map(m1_mapper)\n",
    ")\n",
    "result = toy_electronic.reduceByKey(lambda x,y: x+y)\n",
    "print(result.collect())\n",
    "m1_end = time()\n",
    "\n",
    "print(\"Waktu yang diperlukan dengan Hadoop:\", 97.622-42.046)\n",
    "print(\"Waktu yang diperlukan dengan RDD Spark:\", m1_t1-m1_t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4gk9-rwNakv"
   },
   "source": [
    "#### {tempatkan gambar screenshot yang menunjukkan waktu proses MapReduce Hadoop jar di sini}\n",
    "<img title=\"Waktu Awal\" align=\"left\" src=\"https://cdn.discordapp.com/attachments/997006490877567076/1090508709463072830/image.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img title=\"Waktu Akhir\" align=\"left\" src=\"https://cdn.discordapp.com/attachments/997006490877567076/1090508871124136019/image.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEb0MYjsNakv"
   },
   "source": [
    "## Milestone 2\n",
    "Kerjakan Milestone 2 pada HandsOn Week5 (sebelumnya), akan tetapi menggunakan RDD Spark. Catat waktu (bandingkan) yang dibutuhkan (dalam detik) antara: \"MapReduce menggunakan hadoop streaming jar\" dengan yang akan kamu proses menggunakan RDD Spark ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eQskYTrwNakw",
    "outputId": "05cc1653-2be4-4cf2-ef9e-db92793e75b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('San Francisco', (\"Men's Clothing\", 499.97)), ('Atlanta', ('Pet Supplies', 499.96)), ('Miami', (\"Men's Clothing\", 499.98))]\n",
      "Waktu yang diperlukan dengan Hadoop: 93.344\n",
      "Waktu yang diperlukan dengan RDD Spark: 17.177343130111694\n"
     ]
    }
   ],
   "source": [
    "def m2_mapper(x):\n",
    "    data = x.split(\"\\t\")\n",
    "    try:\n",
    "        return (data[2], (data[3], float(data[4])))\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "m2_start = time()\n",
    "data = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n",
    "mapped = (data\n",
    "    .filter(lambda x: x.split('\\t')[2].lower() in ['miami', 'san francisco', 'atlanta'])\n",
    "    .map(m2_mapper)\n",
    "    .filter(lambda x: x is not None )\n",
    ")\n",
    "result = mapped.reduceByKey(lambda x,y: x if (x[1] > y[1]) else y)\n",
    "print(result.collect())\n",
    "m2_end = time()\n",
    "\n",
    "print(\"Waktu yang diperlukan dengan Hadoop:\", 93.344)\n",
    "print(\"Waktu yang diperlukan dengan RDD Spark:\", m2_end-m2_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeKZRzqCNakx"
   },
   "source": [
    "#### {tempatkan gambar screenshot yang menunjukkan waktu proses MapReduce Hadoop jar di sini}\n",
    "<img title=\"Waktu Awal\" align=\"left\" src=\"https://cdn.discordapp.com/attachments/997006490877567076/1090509113156440074/image.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img title=\"Waktu Akhir\" align=\"left\" src=\"https://cdn.discordapp.com/attachments/997006490877567076/1090510486262526002/image.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aurKREgYNaky"
   },
   "source": [
    "## Milestone 3\n",
    "Kerjakan Milestone 3 pada HandsOn Week5 (sebelumnya), akan tetapi menggunakan RDD Spark. Catat waktu (bandingkan) yang dibutuhkan (dalam detik) antara: \"MapReduce menggunakan hadoop streaming jar\" dengan yang akan kamu proses menggunakan RDD Spark ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RoWcOJtYNakz",
    "outputId": "9be42dd8-b6b5-4f66-e415-82d468164690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('09:01-10:00', 452043), ('10:01-11:00', 452154)]\n",
      "Waktu yang diperlukan dengan Hadoop: 73.182\n",
      "Waktu yang diperlukan dengan RDD Spark: 20.38121008872986\n"
     ]
    }
   ],
   "source": [
    "# Tuliskan code kamu di sini\n",
    "from time import time\n",
    "def m3_mapper(x):\n",
    "    data = x.split(\"\\t\")\n",
    "    _, a_time, _, _, _, _ = data\n",
    "    if (len(data) == 6):\n",
    "        if (a_time >= \"09:01\" and a_time <= \"10.00\" ):\n",
    "            return(\"09:01-10:00\", 1)\n",
    "        elif (a_time >= \"10:01\" and a_time <= \"11.00\" ):\n",
    "            return(\"10:01-11:00\", 1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        \n",
    "m3_t0 = time()\n",
    "data = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n",
    "\n",
    "time_filter = (data.map(mapper).filter(lambda x: x is not None )\n",
    "\n",
    ")\n",
    "\n",
    "result = time_filter.reduceByKey(lambda x,y: x+y)\n",
    "print(result.collect())\n",
    "m3_t1 = time()\n",
    "\n",
    "print(\"Waktu yang diperlukan dengan Hadoop:\", 73.182)\n",
    "print(\"Waktu yang diperlukan dengan RDD Spark:\", m3_t1-m3_t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYXjDgBzNak0"
   },
   "source": [
    "#### {tempatkan gambar screenshot yang menunjukkan waktu proses MapReduce Hadoop jar di sini}\n",
    "<img title=\"Waktu Awal\" align=\"left\" src=\"M3-start.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img title=\"Waktu Akhir\" align=\"left\" src=\"M3-end.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBWr5glANak1"
   },
   "source": [
    "## Milestone 4\n",
    "Milestone ini dibagi menjadi 4.1, 4.2 dan 4.3 yang masing-masing secara berturut-turut adalah mengerjakan ulang Milestone 1, 2 dan 3 di atas (menggunakan RDD Spark), akan tetapi menggunakan trik \"**persist() RDD**\" untuk mempercepat prosesnya. Kamu bisa melakukan \"**persist**\" untuk RDD mana saja yang kamu anggap dapat memberikan waktu proses tercepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "ogD4MxwoNak1",
    "outputId": "450d6cb3-38ff-4021-9fc1-0778e5ac030c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Consumer Electronics', 57452374.12999931), ('Toys', 57463477.10999949)]\n",
      "[('San Francisco', (\"Men's Clothing\", 499.97)), ('Atlanta', ('Pet Supplies', 499.96)), ('Miami', (\"Men's Clothing\", 499.98))]\n",
      "[('09:01-10:00', 452043), ('10:01-11:00', 452154)]\n",
      "Waktu Milestone 1: 21.32858943939209  vs. Waktu Milestone 4.1: 36.74537372589111\n",
      "Waktu Milestone 2: 17.744704246520996  vs. Waktu Milestone 4.2: 9.23516035079956\n",
      "Waktu Milestone 3: 20.38121008872986  vs. Waktu Milestone 4.3: 10.887548923492432\n"
     ]
    }
   ],
   "source": [
    "# Tuliskan code kamu di sini\n",
    "def mapper1(data):\n",
    "    item = data[3]\n",
    "    price =  float(data[4])\n",
    "    if (len(data) == 6):\n",
    "        if (\"toys\" in item.lower()):\n",
    "            item = \"Toys\"\n",
    "            return (item, price)\n",
    "        elif (\"consumer electronics\" in item.lower()):\n",
    "            item = \"Consumer Electronics\"\n",
    "            return (item, price)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def mapper2(data):\n",
    "    _, _, store, item, price, _ = data\n",
    "    price =  float(price)\n",
    "    return (store, (item, price))\n",
    "\n",
    "def mapper3(data):\n",
    "    a_time = data[1]\n",
    "    if (len(data) == 6):\n",
    "        if (a_time >= \"09:01\" and a_time <= \"10.00\" ):\n",
    "            return(\"09:01-10:00\", 1)\n",
    "        elif (a_time >= \"10:01\" and a_time <= \"11.00\" ):\n",
    "            return(\"10:01-11:00\", 1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "data = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n",
    "data = data.map(lambda x : x.split('\\t')).persist()\n",
    "\n",
    "\n",
    "#Milestone 1\n",
    "m1 = data.map(mapper1).filter(lambda x: x is not None)\n",
    "result_m1 = m1.reduceByKey(lambda x, y : x + y)\n",
    "print(result_m1.collect())\n",
    "t1 = time()\n",
    "\n",
    "#Milestone 2\n",
    "m2 = data.filter(lambda x: x[2].lower() in ['miami', 'san francisco', 'atlanta']).map(mapper2)\n",
    "result_m2 = m2.reduceByKey(lambda x,y: x if x[1] > y[1] else y)\n",
    "print(result_m2.collect())\n",
    "t2 = time()\n",
    "#Milestone 3\n",
    "m3 = data.map(mapper3).filter(lambda x: x is not None)\n",
    "result_m3 = m3.reduceByKey(lambda x, y : x + y)\n",
    "print(result_m3.collect())\n",
    "t3 = time()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Waktu Milestone 1:\", m1_t1-m1_t0, \" vs. Waktu Milestone 4.1:\", t1-t0)\n",
    "print(\"Waktu Milestone 2:\", m2_t1-m2_t0, \" vs. Waktu Milestone 4.2:\", t2-t1)\n",
    "print(\"Waktu Milestone 3:\", m3_t1-m3_t0, \" vs. Waktu Milestone 4.3:\", t3-t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-in09VQdNak2"
   },
   "source": [
    "## Milestone 5\n",
    "Milestone ini dibagi menjadi 5.1, 5.2 dan 5.3 yang masing-masing secara berturut-turut adalah mengerjakan ulang Milestone 1, 2 dan 3 di atas, akan tetapi menggunakan DataFrame dari Apache Spark. Catat waktu yang diperlukan untuk masing-masing proses (5.1, 5.2 dan 5.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "KWw3UZWiNak2",
    "outputId": "3963d841-253b-405f-d8d9-5506dd4a0c61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|             product|         sum(price)|\n",
      "+--------------------+-------------------+\n",
      "|Consumer Electronics|5.745237412999931E7|\n",
      "|                Toys|5.746347710999949E7|\n",
      "+--------------------+-------------------+\n",
      "\n",
      "+-------+-------------------+------+\n",
      "|  store|               item| price|\n",
      "+-------+-------------------+------+\n",
      "|Atlanta|               Baby|499.96|\n",
      "|Atlanta|               DVDs|499.96|\n",
      "|Atlanta|                CDs|499.96|\n",
      "|Atlanta|              Music|499.96|\n",
      "|Atlanta|Children's Clothing|499.96|\n",
      "|Atlanta|Children's Clothing|499.96|\n",
      "|Atlanta|     Sporting Goods|499.96|\n",
      "|Atlanta|            Cameras|499.96|\n",
      "|Atlanta|   Women's Clothing|499.96|\n",
      "|Atlanta|              Books|499.96|\n",
      "|Atlanta|Children's Clothing|499.96|\n",
      "|Atlanta|               Baby|499.96|\n",
      "|Atlanta|     Sporting Goods|499.96|\n",
      "|Atlanta|              Music|499.96|\n",
      "|Atlanta|Children's Clothing|499.96|\n",
      "|Atlanta|          Computers|499.96|\n",
      "|Atlanta|Children's Clothing|499.96|\n",
      "|Atlanta|              Books|499.96|\n",
      "|Atlanta|               DVDs|499.96|\n",
      "|Atlanta|               Toys|499.96|\n",
      "+-------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+------+\n",
      "| time_group| count|\n",
      "+-----------+------+\n",
      "|09:01-10:00|459775|\n",
      "|10:01-11:00|459825|\n",
      "+-----------+------+\n",
      "\n",
      "Waktu Milestone 5.1: 11.99240517616272\n",
      "Waktu Milestone 5.2: 25.62139320373535\n",
      "Waktu Milestone 5.3: 12.586020469665527\n"
     ]
    }
   ],
   "source": [
    "# Tuliskan code kamu di sini\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, regexp_replace\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dataframe-spark\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"hdfs://localhost:9000/purchases/purchases.txt\", inferSchema=True, sep=\"\\t\")\n",
    "df = df.selectExpr(\"_c0 as date\", \"_c1 as time\", \"_c2 as store\", \"_c3 as item\", \"_c4 as price\", \"_c5 as payment\")\n",
    "\n",
    "df.createOrReplaceTempView(\"purchases\")\n",
    "\n",
    "t0 = time()\n",
    "spark.sql(\"SELECT (CASE WHEN item LIKE 'Consumer Electronics' THEN 'Consumer Electronics' WHEN item LIKE 'Toys' \\\n",
    "Then 'Toys' Else NULL END) as product, sum(price) \\\n",
    "FROM purchases GROUP BY product \").filter('product is not null').show()\n",
    "t1 = time()\n",
    "maxPrice = spark.sql(\"Select store, max(price) as max_price from purchases group by store\").filter(\"store = 'Miami' or store = 'Atlanta' or store = 'San Francisco'\").createOrReplaceTempView(\"maxPrice\")\n",
    "m2 = spark.sql(\"select maxPrice.store, item, price from purchases inner join maxPrice where price = max_price\").show()\n",
    "t2 = time()\n",
    "m3 = df.filter(\"time <= '11:00' and time >= '09:01'\").selectExpr(\"time <= '10:00' and time >= '09:01' as time_group\", 'item').groupBy('time_group').count()\n",
    "m3 = m3.withColumn(\"time_group\", regexp_replace('time_group', 'true', '09:01-10:00'))\n",
    "m3.withColumn(\"time_group\", regexp_replace('time_group', 'false', '10:01-11:00')).show()\n",
    "t3 = time()\n",
    "print(\"Waktu Milestone 5.1:\", t1-t0)\n",
    "print(\"Waktu Milestone 5.2:\", t2-t1)\n",
    "print(\"Waktu Milestone 5.3:\", t3-t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

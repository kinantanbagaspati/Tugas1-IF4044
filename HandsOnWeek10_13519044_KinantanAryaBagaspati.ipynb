{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uDup6H5Nakk"
   },
   "source": [
    "# HandsOn Week 6 RDD Spark\n",
    "Selamat datang di HandsOn Week 6, yaitu tentang pemrosesan data terdistribusi menggunakan Spark. Untuk tujuan pembelajaran, seperti biasa, kita akan menggunakan *pseudo-distributed mode* (single node cluster) di VM yang telah disediakan. Dengan kode yang *similar* di cluster komputer dengan *n* workers, maka komputasi akan tersebar ke *n* workers tersebut. Adapun yang akan kita coba kali ini adalah melakukan komputasi menggunakan RDD dan DataFrame. Berikut catatan-catatan yang perlu kamu perhatikan dalam hands-on ini:\n",
    "1. Untuk menjalankan Apache Spark dalam bahasa python di VM, ketikkan perintah ```pyspark``` di terminal.\n",
    "2. Dari semua Milestone, data input yang digunakan adalah data \"purchases.txt\" yang telah diletakkan di HDFS. Oleh karena itu, pastikan hadoop service kamu berjalan (```start-dfs.sh```, ```start-yarn.sh```, ```jps```). Untuk membaca data dari HDFS, lihat kembali di slide perkuliahan.\n",
    "3. Untuk Milestone 1, 2 dan 3, kalian perlu untuk mencatat waktu yang diperlukan saat melakukan MapReduce menggunakan hadoop streaming jar di hands-on sebelumnya. Waktu bisa dihitung dari selisih \"waktu awal\" dan \"waktu akhir\" yang tampak di terminal saat kalian selesain melakukan MapReduce -atau menggunakan cara lain yang masih *acceptable*-. (lihat ilustrasi di bawah).\n",
    "4. Lakukan zip file jupyter notebook ini beserta gambar-gambar yang diperlukan -screenshot waktu proses MapReduce Hadoop jar-, dan submit ke portal kuliah EDUNEX dengan format nama \"**HandsOnWeek10_NIM_NamaLengkap.zip**\". Pastikan file jupyter notebook yang kamu zip dalam kondisi memiliki output per cellnya (tidak kosong karena belum dijalankan). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HJOcG2mNaks"
   },
   "source": [
    "## Milestone 1\n",
    "Kerjakan Milestone 1 pada HandsOn Week 6(sebelumnya), akan tetapi menggunakan RDD Spark. Catat waktu (bandingkan) yang dibutuhkan (dalam detik) antara: \"MapReduce menggunakan hadoop streaming jar\" dengan yang akan kamu proses menggunakan RDD Spark ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "caB7L1cKNaks",
    "outputId": "5f26548d-0307-412c-939b-5ec799f6bab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Consumer Electronics', 57452374.12999931), ('Toys', 57463477.10999949)]\n",
      "Waktu yang diperlukan dengan Hadoop: 55.576\n",
      "Waktu yang diperlukan dengan RDD Spark: 31.404622077941895\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "def m1_mapper(x):\n",
    "    data = x.split(\"\\t\")\n",
    "    try:\n",
    "        if (\"toys\" in data[3].lower()):\n",
    "            return (\"Toys\", float(data[4]))\n",
    "        elif (\"consumer electronics\" in data[3].lower()):\n",
    "            return (\"Consumer Electronics\", float(data[4]))\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "m1_start = time()\n",
    "data = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n",
    "mapped_data = (data.filter(\n",
    "        lambda x: (\"toys\" in x.split(\"\\t\")[3].lower()) or (\"consumer electronics\" in x.split(\"\\t\")[3].lower())\n",
    "    ).map(m1_mapper)\n",
    ")\n",
    "result = toy_electronic.reduceByKey(lambda x,y: x+y)\n",
    "print(result.collect())\n",
    "m1_end = time()\n",
    "\n",
    "print(\"Waktu yang diperlukan dengan Hadoop:\", 97.622-42.046)\n",
    "print(\"Waktu yang diperlukan dengan RDD Spark:\", m1_t1-m1_t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4gk9-rwNakv"
   },
   "source": [
    "#### {tempatkan gambar screenshot yang menunjukkan waktu proses MapReduce Hadoop jar di sini}\n",
    "<img title=\"Waktu Awal\" align=\"left\" src=\"https://cdn.discordapp.com/attachments/997006490877567076/1090508709463072830/image.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img title=\"Waktu Akhir\" align=\"left\" src=\"https://cdn.discordapp.com/attachments/997006490877567076/1090508871124136019/image.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEb0MYjsNakv"
   },
   "source": [
    "## Milestone 2\n",
    "Kerjakan Milestone 2 pada HandsOn Week5 (sebelumnya), akan tetapi menggunakan RDD Spark. Catat waktu (bandingkan) yang dibutuhkan (dalam detik) antara: \"MapReduce menggunakan hadoop streaming jar\" dengan yang akan kamu proses menggunakan RDD Spark ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eQskYTrwNakw",
    "outputId": "05cc1653-2be4-4cf2-ef9e-db92793e75b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('San Francisco', (\"Men's Clothing\", 499.97)), ('Atlanta', ('Pet Supplies', 499.96)), ('Miami', (\"Men's Clothing\", 499.98))]\n",
      "Waktu yang diperlukan dengan Hadoop: 53.397\n",
      "Waktu yang diperlukan dengan RDD Spark: 11.250900745391846\n"
     ]
    }
   ],
   "source": [
    "def m2_mapper(x):\n",
    "    data = x.split(\"\\t\")\n",
    "    try:\n",
    "        if data[2].lower() in ['miami', 'san francisco', 'atlanta']:\n",
    "            return (data[2], (data[3], float(data[4])))\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "m2_start = time()\n",
    "data = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n",
    "mapped = (data\n",
    "    .filter(lambda x: x.split('\\t')[2].lower() in ['miami', 'san francisco', 'atlanta'])\n",
    "    .map(m2_mapper)\n",
    "    .filter(lambda x: x is not None )\n",
    ")\n",
    "result = mapped.reduceByKey(lambda x,y: x if (x[1] > y[1]) else y)\n",
    "print(result.collect())\n",
    "m2_end = time()\n",
    "\n",
    "print(\"Waktu yang diperlukan dengan Hadoop:\", 56.833 - 3.436)\n",
    "print(\"Waktu yang diperlukan dengan RDD Spark:\", m2_end-m2_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeKZRzqCNakx"
   },
   "source": [
    "#### {tempatkan gambar screenshot yang menunjukkan waktu proses MapReduce Hadoop jar di sini}\n",
    "<img title=\"Waktu Awal\" align=\"left\" src=\"https://cdn.discordapp.com/attachments/997006490877567076/1090509113156440074/image.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img title=\"Waktu Akhir\" align=\"left\" src=\"https://cdn.discordapp.com/attachments/997006490877567076/1090510486262526002/image.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aurKREgYNaky"
   },
   "source": [
    "## Milestone 3\n",
    "Kerjakan Milestone 3 pada HandsOn Week5 (sebelumnya), akan tetapi menggunakan RDD Spark. Catat waktu (bandingkan) yang dibutuhkan (dalam detik) antara: \"MapReduce menggunakan hadoop streaming jar\" dengan yang akan kamu proses menggunakan RDD Spark ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "RoWcOJtYNakz",
    "outputId": "9be42dd8-b6b5-4f66-e415-82d468164690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('09:01-10:00', 452043), ('10:01-11:00', 452154)]\n",
      "Waktu yang diperlukan dengan Hadoop: 58.30899999999999\n",
      "Waktu yang diperlukan dengan RDD Spark: 10.184515953063965\n"
     ]
    }
   ],
   "source": [
    "# Tuliskan code kamu di sini\n",
    "from time import time\n",
    "def m3_mapper(x):\n",
    "    data = x.split(\"\\t\")\n",
    "    if (data[1] >= \"09:01\" and data[1] <= \"10.00\" ):\n",
    "        return(\"09:01-10:00\", 1)\n",
    "    elif (data[1] >= \"10:01\" and data[1] <= \"11.00\" ):\n",
    "        return(\"10:01-11:00\", 1)\n",
    "    return None\n",
    "\n",
    "        \n",
    "m3_start = time()\n",
    "data = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n",
    "\n",
    "time_filter = (data\n",
    "    .filter(lambda x: (\"09:01\" <= x.split(\"\\t\")[1]) and (x.split(\"\\t\")[1] <= \"11:00\"))\n",
    "    .map(m3_mapper)\n",
    "    .filter(lambda x: x is not None)\n",
    ")\n",
    "\n",
    "result = time_filter.reduceByKey(lambda x,y: x+y)\n",
    "print(result.collect())\n",
    "m3_end = time()\n",
    "\n",
    "print(\"Waktu yang diperlukan dengan Hadoop:\", 103.835 - 45.526)\n",
    "print(\"Waktu yang diperlukan dengan RDD Spark:\", m3_end-m3_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYXjDgBzNak0"
   },
   "source": [
    "#### {tempatkan gambar screenshot yang menunjukkan waktu proses MapReduce Hadoop jar di sini}\n",
    "<img title=\"Waktu Awal\" align=\"left\" src=\"https://cdn.discordapp.com/attachments/997006490877567076/1090510611923869736/image.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img title=\"Waktu Akhir\" align=\"left\" src=\"https://cdn.discordapp.com/attachments/997006490877567076/1090510729746071652/image.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBWr5glANak1"
   },
   "source": [
    "## Milestone 4\n",
    "Milestone ini dibagi menjadi 4.1, 4.2 dan 4.3 yang masing-masing secara berturut-turut adalah mengerjakan ulang Milestone 1, 2 dan 3 di atas (menggunakan RDD Spark), akan tetapi menggunakan trik \"**persist() RDD**\" untuk mempercepat prosesnya. Kamu bisa melakukan \"**persist**\" untuk RDD mana saja yang kamu anggap dapat memberikan waktu proses tercepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ogD4MxwoNak1",
    "outputId": "450d6cb3-38ff-4021-9fc1-0778e5ac030c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Consumer Electronics', 57452374.12999931), ('Toys', 57463477.10999949)]\n",
      "[('San Francisco', (\"Men's Clothing\", 499.97)), ('Atlanta', ('Pet Supplies', 499.96)), ('Miami', (\"Men's Clothing\", 499.98))]\n",
      "[('09:01-10:00', 452043), ('10:01-11:00', 452154)]\n",
      "M1: 22.396260023117065  vs M4.1: 12.49790096282959\n",
      "M2: 11.250900745391846  vs M4.2: 9.130762100219727\n",
      "M3: 10.184515953063965  vs M4.3: 9.907243967056274\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "data = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\").persist()\n",
    "\n",
    "mapped1 = data.map(m1_mapper).filter(lambda x: x is not None)#.persist()\n",
    "result_m1 = mapped1.reduceByKey(lambda x, y : x + y)\n",
    "print(result_m1.collect())\n",
    "t1 = time()\n",
    "\n",
    "mapped2 = data.map(m2_mapper).filter(lambda x: x is not None)#.persist()\n",
    "result_m2 = mapped2.reduceByKey(lambda x,y: x if x[1] > y[1] else y)\n",
    "print(result_m2.collect())\n",
    "t2 = time()\n",
    "\n",
    "mapped3 = data.map(m3_mapper).filter(lambda x: x is not None)#.persist()\n",
    "result_m3 = mapped3.reduceByKey(lambda x, y : x + y)\n",
    "print(result_m3.collect())\n",
    "t3 = time()\n",
    "\n",
    "print(\"M1:\", m1_end - m1_start, \" vs M4.1:\", t1 - t0)\n",
    "print(\"M2:\", m2_end - m2_start, \" vs M4.2:\", t2 - t1)\n",
    "print(\"M3:\", m3_end - m3_start, \" vs M4.3:\", t3 - t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-in09VQdNak2"
   },
   "source": [
    "## Milestone 5\n",
    "Milestone ini dibagi menjadi 5.1, 5.2 dan 5.3 yang masing-masing secara berturut-turut adalah mengerjakan ulang Milestone 1, 2 dan 3 di atas, akan tetapi menggunakan DataFrame dari Apache Spark. Catat waktu yang diperlukan untuk masing-masing proses (5.1, 5.2 dan 5.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "KWw3UZWiNak2",
    "outputId": "3963d841-253b-405f-d8d9-5506dd4a0c61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|              barang|       jumlah_harga|\n",
      "+--------------------+-------------------+\n",
      "|Consumer Electronics|5.745237412999931E7|\n",
      "|                Toys|5.746347710999949E7|\n",
      "+--------------------+-------------------+\n",
      "\n",
      "+-------------+-------------+--------------+\n",
      "|         kota|max_penjualan|    min_barang|\n",
      "+-------------+-------------+--------------+\n",
      "|        Miami|       499.98|Men's Clothing|\n",
      "|San Francisco|       499.97|Men's Clothing|\n",
      "|      Atlanta|       499.96|  Pet Supplies|\n",
      "+-------------+-------------+--------------+\n",
      "\n",
      "+------+--------+\n",
      "|cutoff|count(1)|\n",
      "+------+--------+\n",
      "|  true|  459825|\n",
      "| false|  459775|\n",
      "+------+--------+\n",
      "\n",
      "M5.1: 7.472270488739014\n",
      "M5.2: 21.279218435287476\n",
      "M5.3: 7.105727195739746\n"
     ]
    }
   ],
   "source": [
    "# Tuliskan code kamu di sini\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, regexp_replace\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"dataframe-spark\")\n",
    "    .config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    ")\n",
    "\n",
    "df = spark.read.csv(\"hdfs://localhost:9000/purchases/purchases.txt\", inferSchema=True, sep=\"\\t\")\n",
    "df = df.selectExpr(\"_c0 as tanggal\", \"_c1 as jam\", \"_c2 as kota\", \"_c3 as barang\", \"_c4 as harga\", \"_c5 as bayar\")\n",
    "\n",
    "df.createOrReplaceTempView(\"purchases\")\n",
    "t0 = time()\n",
    "\n",
    "spark.sql('''\n",
    "SELECT barang, sum(harga) AS jumlah_harga\n",
    "FROM (SELECT barang, harga FROM purchases WHERE barang LIKE '%Consumer Electronics%' OR barang LIKE '%Toys%') AS filtered_purchases\n",
    "GROUP BY barang\n",
    "''').show()\n",
    "t1 = time()\n",
    "\n",
    "#max_purchases = spark.sql(\"Select toko, max(harga) as max_price from purchases group by store\").filter(\"store = 'Miami' or store = 'Atlanta' or store = 'San Francisco'\").createOrReplaceTempView(\"maxPrice\")\n",
    "spark.sql('''\n",
    "WITH filtered_purchases AS (\n",
    "SELECT kota, harga, barang FROM purchases\n",
    "WHERE kota = 'Miami' OR kota = 'San Francisco' OR kota = 'Atlanta'\n",
    "),\n",
    "max_purchases AS (\n",
    "SELECT kota, MAX(harga) AS max_penjualan\n",
    "FROM filtered_purchases GROUP BY kota\n",
    ")\n",
    "SELECT filtered_purchases.kota, max_penjualan, MIN(barang) AS min_barang\n",
    "FROM filtered_purchases JOIN max_purchases\n",
    "ON filtered_purchases.kota = max_purchases.kota\n",
    "AND filtered_purchases.harga = max_purchases.max_penjualan\n",
    "GROUP BY filtered_purchases.kota, max_penjualan\n",
    "''').show()\n",
    "t2 = time()\n",
    "\n",
    "spark.sql('''\n",
    "SELECT cutoff, COUNT(*) FROM (\n",
    "SELECT (jam > '10:00') AS cutoff, *\n",
    "FROM (SELECT * FROM purchases WHERE jam > '09:00' AND jam <= '11:00')\n",
    ") AS temporary GROUP BY cutoff\n",
    "''').show()\n",
    "t3 = time()\n",
    "\n",
    "print(\"M5.1:\", t1-t0)\n",
    "print(\"M5.2:\", t2-t1)\n",
    "print(\"M5.3:\", t3-t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

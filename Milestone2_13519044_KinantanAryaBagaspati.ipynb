{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "caB7L1cKNaks",
    "outputId": "5f26548d-0307-412c-939b-5ec799f6bab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done reading\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from time import time\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from json import load, loads\n",
    "import csv, io\n",
    "\n",
    "rdd = sc.wholeTextFiles(\"hdfs://localhost:9000/raw_json\")\n",
    "\n",
    "facebook_data = rdd.filter(lambda x: \"facebook\" in x[0]).flatMap(lambda x: loads(x[1])).persist()\n",
    "twitter_data = rdd.filter(lambda x: \"twitter\" in x[0]).flatMap(lambda x: loads(x[1])).persist()\n",
    "youtube_data = rdd.filter(lambda x: \"youtube\" in x[0]).flatMap(lambda x: loads(x[1])).persist()\n",
    "instagram_data = rdd.filter(lambda x: \"instagram\" in x[0]).flatMap(lambda x: loads(x[1])).persist()\n",
    "\n",
    "print(\"done reading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDFS digunakan sebagai backgroundservice sumber data dari Spark. Tiap pembacaan memiliki info nama file dan isi file, yang saya gunakan untuk membedakan sosial media tiap entry data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eQskYTrwNakw",
    "outputId": "05cc1653-2be4-4cf2-ef9e-db92793e75b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('facebook', '2021-01-30', 1, 16), ('facebook', '2021-01-28', 1, 25), ('facebook', '2021-01-25', 1, 21), ('facebook', '2021-01-21', 1, 35), ('facebook', '2021-01-19', 4, 101), ('facebook', '2021-01-18', 4, 91), ('facebook', '2021-01-15', 3, 62), ('facebook', '2021-01-10', 1, 26), ('facebook', '2021-01-02', 1, 23), ('facebook', '2021-02-28', 1, 27)]\n"
     ]
    }
   ],
   "source": [
    "def post_mapper(post):\n",
    "    try:\n",
    "        return(post.get(\"created_time\", \"-\").split(\"T\")[0], (1, 0))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "def comment_mapper(comment):\n",
    "    try:\n",
    "        return (comment.get(\"created_time\", \"-\").split(\"T\")[0], (0, 1))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "post_mapped = facebook_data.map(lambda post: post_mapper(post)).persist()\n",
    "\n",
    "comment_data = facebook_data.flatMap(lambda x: x.get(\"comments\", {}).get(\"data\", []))\n",
    "comment_mapped = comment_data.map(lambda comment: comment_mapper(comment)).persist()\n",
    "\n",
    "facebook_mapped = post_mapped.union(comment_mapped)\n",
    "facebook_reduced = (facebook_mapped\n",
    "    .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "    .map(lambda x: (\"facebook\", x[0], x[1][0], x[1][1]))\n",
    ")\n",
    "print(facebook_reduced.collect()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pemrosesan data facebook untuk setiap entrynya berupa suatu post, juga merupakan sarang dari sejumlah comment. Comment-comment ini juga tidak musti pada hari yang sama, oleh karena itu perlu dimasukkan dalam proses mapreduce dengan logika lain. Saya memanfaatkan flatMap untuk memproses comment untuk akhirnya menghasilkan 3-tuple sama dengan hasil post, yakni (tanggal, nb_post, nb_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RoWcOJtYNakz",
    "outputId": "9be42dd8-b6b5-4f66-e415-82d468164690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('twitter', '2021-01-01', 11, 637), ('twitter', '2021-01-10', 5, 670), ('twitter', '2021-01-16', 1, 1), ('twitter', '2021-01-18', 7, 1816), ('twitter', '2021-01-19', 24, 1242), ('twitter', '2021-02-05', 185, 1685), ('twitter', '2021-01-29', 3, 2), ('twitter', '2021-01-31', 7, 482), ('twitter', '2021-01-28', 4, 6), ('twitter', '2021-02-06', 17, 14)]\n"
     ]
    }
   ],
   "source": [
    "def twitter_mapper(data):\n",
    "    date = datetime.strptime(data.get('created_at'), \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    try:\n",
    "        return (date.strftime('%Y-%m-%d'), (1, data.get(\"reply_count\", 0)))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "twitter_mapped = twitter_data.map(lambda post: twitter_mapper(post)).persist()\n",
    "twitter_reduced = (twitter_mapped\n",
    "    .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "    .map(lambda x: (\"twitter\", x[0], x[1][0], x[1][1]))\n",
    ")\n",
    "print(twitter_reduced.collect()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setiap entry data merupakan post twitter yang memiliki data jumlah reply (saya anggap reply sebagai comment). Tanggal saya ambil dari salah satu fungsi bawaan datetime untuk mengganti format agar sesuai sosial media lain. Sesuai umumnya pada tugas ini, mapper menghasilkan 3-tuple (tanggal, nb_post, nb_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ogD4MxwoNak1",
    "outputId": "450d6cb3-38ff-4021-9fc1-0778e5ac030c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('youtube', '2021-07-26', 1, 10), ('youtube', '2021-07-22', 0, 8), ('youtube', '2021-07-15', 0, 8), ('youtube', '2021-06-27', 0, 6), ('youtube', '-', 0, 1738), ('youtube', '2021-06-23', 0, 10), ('youtube', '2021-06-17', 0, 12), ('youtube', '2021-06-14', 1, 6), ('youtube', '2021-06-12', 0, 172), ('youtube', '2021-06-11', 0, 4)]\n"
     ]
    }
   ],
   "source": [
    "def youtube_mapper(data):\n",
    "    date_str = data.get(\"snippet\", {}).get(\"publishedAt\", \"-\").split(\"T\")[0]\n",
    "    if(data.get(\"kind\") == \"youtube#video\"):\n",
    "        return (date_str, (1, 0))\n",
    "    else:\n",
    "        return (date_str, (0, 1))\n",
    "\n",
    "    \n",
    "youtube_mapped = youtube_data.map(lambda post: youtube_mapper(post)).persist()\n",
    "youtube_reduced = (youtube_mapped\n",
    "    .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "    .map(lambda x: (\"youtube\", x[0], x[1][0], x[1][1]))\n",
    ")\n",
    "print(youtube_reduced.collect()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youtube post atau comment dibedakan dari kindnya, youtube#video sebagai post atau youtube#comment sebagai comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KWw3UZWiNak2",
    "outputId": "3963d841-253b-405f-d8d9-5506dd4a0c61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('instagram', '2021-03-20', 6, 86), ('instagram', '2021-03-21', 0, 38), ('instagram', '2021-03-22', 14, 36), ('instagram', '2021-03-29', 0, 5), ('instagram', '2021-04-09', 19, 14), ('instagram', '2021-04-22', 4, 5), ('instagram', '2021-01-04', 8, 2), ('instagram', '2021-01-15', 13, 12), ('instagram', '2021-01-16', 2, 83), ('instagram', '2021-01-17', 0, 24)]\n"
     ]
    }
   ],
   "source": [
    "def instagram_mapper(data):\n",
    "    date_str = \"-\"\n",
    "    try:\n",
    "        timestamp = int(data.get(\"created_time\"))\n",
    "        if(timestamp > 0):\n",
    "            date_str = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        pass\n",
    "    if data.get(\"parent\"): # This is a comment\n",
    "        return (date_str, (0, 1))\n",
    "    else: # This is a post\n",
    "        return (date_str, (1, 0))\n",
    "\n",
    "instagram_mapped = instagram_data.map(lambda post: instagram_mapper(post)).persist()\n",
    "instagram_reduced = (instagram_mapped\n",
    "    .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "    .map(lambda x: (\"instagram\", x[0], x[1][0], x[1][1]))\n",
    ")\n",
    "print(instagram_reduced.collect()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saya membedakan apakah data instagram merupakan post atau comment dari apakah memiliki atribut \"parent\" yang berarti merupakan comment. Sebetulnya dalam post ada juga kumpulan comment, namun saya cek data yang sama dengan komen tersebut muncul lagi di entry berbeda, sehingga saya rasa pendekatan ini sudah sesuai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_csv_str(x):\n",
    "    output = io.StringIO(\"\")\n",
    "    csv.writer(output).writerow(x)\n",
    "    return output.getvalue().strip()\n",
    "\n",
    "result = facebook_reduced.union(twitter_reduced).union(youtube_reduced).union(instagram_reduced)\n",
    "result_as_csv = result.map(list_to_csv_str)\n",
    "result_as_csv.saveAsTextFile(\"hdfs://localhost:9000/output2/milestone2_result\")\n",
    "result_as_csv.saveAsTextFile(\"milestone2_result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simpan ke dalam folder milestone2 setelah digabung. Sekali lagi dijelaskan terdapat 4 kolom, yakni berturut-turut jenis media sosial, tanggal, banyak post pada tanggal tersebut, dan banyak komen pada tanggal tersebut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
